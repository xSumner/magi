### 1. 简介

定量的数据是用来度量某个对象的，比如一个班有多少人、每月的销售额或者学生的成绩等。

本文会讲解多种将原始数值型数据转换成机器学习算法所需特征的方法，主要用到的是 Python 机器学习包 Scikit-learn 包中的预处理功能。

### 2. 特征的缩放

用 scikit-learn 的 MinMaxScaler 将一个数值型特征的值缩放（rescale）到两个特定的值之间：

```python
# 加载库
import numpy as np
from sklearn import preprocessing

# 创建特征
feature = np.array([[-500.5],
                    [-100.1],
                    [0],
                    [100.1],
                    [900.9]])

# 创建缩放器
minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))

# 缩放特征的值
scaled_feature = minmax_scale.fit_transform(feature)

# 查看特征
scaled_feature

# array([[ 0. ],
#        [ 0.28571429],
#        [ 0.35714286],
#        [ 0.42857143],
#        [ 1. ]])
```

在机器学习中，缩放是一个很常见的预处理任务。min-max 缩放利用特征的最小值和最大值，将所有特征都缩放到同一个范围中。

其计算公式如下 ：
$$
x_i^{'}=\frac{x_i-\min(x)}{\max(x)-\min(x)}
$$
从本例输出的数组中可以看出，特征的元素值已经被成功地缩放到 0 和 1 之间了 。



### 3. 特征的标准化

scikit-learn 的 StandardScaler 能对一个特征进行转换，使其平均值为 0、标准差为 1 ：

```python
# 加载库
import numpy as np
from sklearn import preprocessing

# 创建特征
x = np.array([[-1000.1],
              [-200.2],
              [500.5],
              [600.6],
              [9000.9]])

# 创建缩放器
scaler = preprocessing.StandardScaler()

# 转换特征
standardized = scaler.fit_transform(x)

# 查看特征
standardized

# array([[-0.76058269],
#        [-0.54177196],
#        [-0.35009716],
#        [-0.32271504],
#        [ 1.97516685]])
```

为了实现将特征缩放为大致符合标准正态分布的，可以使用标准化（standardization）方法来转换数据，这样特征中的每个元素都会被转换，使得 ：
$$
x^′_i =\frac{x_i − \bar{x}}{σ}
$$
转换后的特征表示原始值距离平均值多少个标准差（在统计学中也称为 z 分数）。

查看最后输出的平均值和标准差：

```python
# 打印平均值和标准差
print("Mean:" , round(standardized.mean()))
print("Standard deviation:" , standardized.std())

# Mean: 0.0
# Standard deviation: 1.0
```

如果数据中存在很大的异常值，可能会影响特征的平均值和方差，也会对标准化的效果造成不良影响。

这种情况下，使用中位数和四分位数间距进行缩放会更有效。

在scikit-learn 中，具体的做法就是调用 RobustScaler ：

```python
# 创建缩放器
robust_scaler = preprocessing.RobustScaler()

# 转换特征
robust_scaler.fit_transform(x)

# array([[ -1.87387612],
#        [ -0.875 ],
#        [ 0. ],
#        [ 0.125 ],
#        [ 10.61488511]])
```

### 4. 归一化观察值

使用 Normalizer 并指定 norm 参数，对观察值的每一个特征进行缩放，使其拥有一致的范数（总长度是 1） ：

```python
# 加载库
import numpy as np
from sklearn.preprocessing import Normalizer

# 创建特征矩阵
features = np.array([[0.5, 0.5],
                     [1.1, 3.4],
                     [1.5, 20.2],
                     [1.63, 34.4],
                     [10.9, 3.3]])

# 创建归一化器
normalizer = Normalizer(norm="l2")

# 转换特征矩阵
normalizer.transform(features)

# array([[ 0.70710678, 0.70710678],
#        [ 0.30782029, 0.95144452],
#        [ 0.07405353, 0.99725427],
#        [ 0.04733062, 0.99887928],
#        [ 0.95709822, 0.28976368]])
```

除了对特征进行缩放操作，也可以对观察值也可以进行缩放操作。 Normalizer 可以对单个观察值进行缩放，使其拥有一致的范数（总长度为 1）。

当一个观察值有多个相等的特征时（例如，做文本分类时，每一个词或每几个词就是一个特征），经常使用这种类型的缩放。

Normalizer 提供三个范数选项，默认值是欧氏范数（Euclidean norm，常被称为 L2 范数）：
$$
||x||_2 = \sqrt{x_1^2+x_2^2+\cdots+x_n^2}
$$
或者，可以指定使用曼哈顿范数（L1 范数）：
$$
||x||_1 = \sum_{i=1}^n |x_i|
$$

```python
# 转换特征矩阵
features_l1_norm = Normalizer(norm="l1").transform(features)

# 查看特征矩阵
features_l1_norm

# array([[ 0.5 , 0.5 ],
#        [ 0.24444444, 0.75555556],
#        [ 0.06912442, 0.93087558],
#        [ 0.04524008, 0.95475992],
#        [ 0.76760563, 0.23239437]])
```

直观地说， L2 范数可以被视为纽约市中两个点之间的距离（也就是直线距离），而 L1 范数可以被视为一个人沿着街道行走的距离（向北走一个街区，向东走一个街区，然后向北走一个街区，再向东走一个街区，等等），这就是为什么它被称为"曼哈顿范数"或者"出租车范数"的原因。

实际上，当你用 norm='l1' 对一个观察值进行缩放后，它的元素总和为 1。有时候这个特性很有用 ：

```python
# 查看总和
print("Sum of the first observation\'s values:",
      features_l1_norm[0, 0] + features_l1_norm[0, 1])

# Sum of the first observation's values: 1.0
```

### 5. 生成多项式和交互特征

除了手动创建多项式特征和交互特征，scikit-learn 提供了一个内置的方法创建多项式特征和交互特征 ：

```python
# 加载库
import numpy as np
from sklearn.preprocessing import PolynomialFeatures

# 创建特征矩阵
features = np.array([[2, 3],
                     [2, 3],
                     [2, 3]])

# 创建 PolynomialFeatures 对象
polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)

# 创建多项式特征
polynomial_interaction.fit_transform(features)

# array([[ 2., 3., 4., 6., 9.],
#        [ 2., 3., 4., 6., 9.],
#        [ 2., 3., 4., 6., 9.]])
```

degree 参数决定了多项式的最高阶数。例如， degree=2 会创建阶数最高为 2 的特征 ：
$$
x_1,x_2,x_1^2,x_2^2
$$
degree=3 则会创建阶数最高为 3 的特征 ：
$$
x_1,x_2,x_1^2,x_2^2,x_1^3,x_2^3
$$
此外，默认情况下 PolynomialFeatures 包含交互特征 ：
$$
x_1x_2
$$
通过设置 interaction_only 为 True，可以强制创建出来的特征只包含交互特征 ：

```python
interaction = PolynomialFeatures(degree=2,
                                 interaction_only=True, include_bias=False)

interaction.fit_transform(features)

# array([[ 2., 3., 6.],
#        [ 2., 3., 6.],
#        [ 2., 3., 6.]])
```

当特征和目标值（预测值）之间存在非线性关系时，就需要创建多项式特征。例如，年龄可能和身体状况有很大的关系，一般年龄越大身体状况越差，但是它们之间的关系不是线性关系，所以需要对特征 $x$ 编码——生成这个特征的更高阶的形式（$x_2$、 $x_3$ 等）—— 以此表示对目标值造成的非线性影响。

另外，有时我们会遇到一个特征需要依赖于另一个特征才能对目标值造成影响的情况。举一个简单的例子，要预测一杯咖啡是否是甜的，要考虑两个特征 ：咖啡是否被搅拌过 ；咖啡是否加了糖。单独看二者中的任何一个，都无法确定这杯咖啡是甜的，但是把它们结合在一起，就能确定了。咖啡只有在加糖并且搅拌后才是甜的，这两个特征对目标值（咖啡是甜的）的作用是相互依赖的。生成一个交互特征（将两个特征相乘），我们就可以为这种关系编码。

### 6. 转换特征

在 scikit-learn 中，使用 FunctionTransformer 对一个或多个特征进行自定义的转换 ：

```python
# 加载库
import numpy as np
from sklearn.preprocessing import FunctionTransformer

# 创建特征矩阵
features = np.array([[2, 3],
                     [2, 3],
                     [2, 3]])

# 定义一个简单的函数
def add_ten(x):
    return x + 10

# 创建转换器
ten_transformer = FunctionTransformer(add_ten)

# 转换特征矩阵
ten_transformer.transform(features)

# array([[12, 13],
#        [12, 13],
#        [12, 13]])
```

在 pandas 中可以使用 apply 进行同样的转换 ：

```python
# 加载库
import pandas as pd

# 创建数据帧
df = pd.DataFrame(features, columns=["feature_1" ,"feature_2" ])

# 应用函数
df.apply(add_ten)

#   feature_1 feature_2
# 0 12 13
# 1 12 13
# 2 12 13
```

对一个或多个特征进行自定义的转换是很常见操作。例如，你可能想创建一个特征来表示另一个特征的自然对数值。创建一个函数，并使用 scikit-learn 的FunctionTransformer 或 pandas 的 apply 来应用这个函数，就能做到这一点。上述解决方案中创建了一个很简单的函数 add_ten，它将每一个输入值加 10。当然，我们也完全可以定义一个复杂得多的函数。

### 7. 识别异常值

与其说识别样本中的一些极端观察值——异常值（outlier）是一门技术，不如说它是一门艺术。常用的方法是假设数据是正态分布的，基于这个假设，在数据周围"画"一个椭圆，将所有处于椭圆内的观察值视为正常值（标注为 1），将所有处于椭圆外的观察值视为异常值（标注为 -1）：

```python
# 加载库
import numpy as np
from sklearn.covariance import EllipticEnvelope
from sklearn.datasets import make_blobs

# 创建模拟数据
features, _ = make_blobs(n_samples = 10,
                         n_features = 2,
                         centers = 1,
                         random_state = 1)

# 将第一个观察值的值替换为极端值
features[0,0] = 10000
features[0,1] = 10000

# 创建识别器
outlier_detector = EllipticEnvelope(contamination=.1)

# 拟合识别器
outlier_detector.fit(features)

# 预测异常值
outlier_detector.predict(features)

# array([-1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
```

这个方法的一个主要限制是它需要指定一个 contamination（污染指数）参数，表示异常值在观察值中的比例——这个值我们也不知道是多少。可以将 contamination视为你估计的数据的清洁程度。如果你认为数据中只有很少几个异常值，可以将contamination 设置得小一点。反之，如果数据中很有可能有好几个异常值，就将contamination 设置为一个更大的值。

除了查看所有观察值，我们还可以只查看某些特征，并使用四分位差（interqutile range，IQR）来识别这些特征的极端值。

```python
# 创建一个特征
feature = features[:,0]

# 创建一个函数来返回异常值的下标
def indicies_of_outliers(x):
    q1, q3 = np.percentile(x, [25, 75])
    iqr = q3 - q1
    lower_bound = q1 - (iqr * 1.5)
    upper_bound = q3 + (iqr * 1.5)
    return np.where((x > upper_bound) | (x < lower_bound))

# 执行函数
indicies_of_outliers(feature)

# (array([0], dtype=int64),)
```

IQR 是数据集的第 1 个四分位数和第 3 个四分位数之差。可以将 IQR 视为数据集中大部分数据的延展距离，而异常值会远远地偏离数据较为集中的区域。异常值常常被定义为比第 1 个四分位数小 1.5 IQR（即 IQR 的 1.5 倍）的值，或比第 3 个四分位数大 1.5 IQR的值。

实际上没有一个通用的识别异常值的解决方案。每一种技术都有它的优点和缺点。最好的策略是尝试多种技术（比如 EllipticEnvelope 和基于 IQR 的识别）并从整体上来看结果。

如果可能的话，你应该仔细看一看被识别为异常值的观察值，并尝试去解释它们。例如，假设你有一个关于房子的数据集，其中一个特征是房间数。如果一个房子拥有 100 个房间，那么这个房子真的是一个异常值，还是说它实际上是旅馆而被错误地划归为异常值了呢？

### 8. 处理异常值

通常有三种方式来处理数据中的异常值。第一种方式，丢弃它们 ：

```python
# 加载库
import pandas as pd

# 创建数据帧
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]

# 筛选观察值
houses[houses['Bathrooms'] < 20]

# Price Bathrooms Square_Feet
# 0 534433 2.0 1500
# 1 392333 3.5 2500
# 2 293222 2.0 1500
```

第二种方式，将它们标记为异常值，并作为数据的一个特征 ：

```python
# 加载库
import numpy as np

# 基于布尔条件语句来创建特征
houses["Outlier"] = np.where(houses["Bathrooms"] < 20, 0, 1)

# 查看数据
houses

# Price Bathrooms Square_Feet Outlier
# 0 534433 2.0 1500 0
# 1 392333 3.5 2500 0
# 2 293222 2.0 1500 0
# 3 4322032 116.0 48000 1
```

最后，对有异常值的特征进行转换，降低异常值的影响 ：

```python
# 对特征取对数值
houses["Log_Of_Square_Feet"] = [np.log(x) for x in houses["Square_Feet"]]

# 查看数据
houses

# Price Bathrooms Square_Feet Outlier Log_Of_Sqaure_Feet
# 0 534433 2.0 1500 0 7.313220
# 1 392333 3.5 2500 0 7.824046
# 2 293222 2.0 1500 0 7.313220
# 3 4322032 116.0 48000 1 10.778956
```

和识别异常值一样，处理异常值时也不存在一个绝对准则。应该基于两个方面来考虑对异常值的处理。第一，要弄清楚是什么让它们成为异常值的。如果你认为它们是错误的观察值，比如它们来自一个坏掉的传感器或者是被记错了的值，那么就要丢弃它们或者用 NaN 来替换异常值，因为我们无法信任这些值。但是，如果你认为这些异常值真的就是极端值（例如一幢大宅子有 200 间卧室），那么把它们标记为异常值或者对它们的值进行转换，是更合理的做法。

第二，应该基于机器学习的目标来处理异常值。例如，如果想要基于房屋的特征来预测其价格，那么可以合理地假设有 100 间卧室的大宅子的价格是由不同于普通家庭住宅的特征驱动的。此外，如果使用一个在线住房贷款的 Web 应用的部分数据来训练一个模型，那么就要假设潜在用户中不存在想要买一栋有几百间卧室的豪宅的亿万富翁。

所以，对于异常值到底要如何处理呢？首先，想一想它们为什么是异常值，然后对于数据要有一个最终的目标。最重要的是，要记住"决定不处理异常值"本身就是一个有潜在影响的决定。

另外，如果数据中有异常值，那么采用标准化方法做缩放就不太合适了，因为平均值和方差受异常值的影响很大。这种情况下，需要针对异常值使用一个鲁棒性更高的缩放方法，比如 RobustScaler。

### 9. 将特征离散化

特征离散化是指将一个数值型特征离散化，分到多个离散的小区间中。根据数据离散化的方式，有两种方法可以使用。第一种方法是根据阈值将特征二值化 ：

```python
# 加载库
import numpy as np
from sklearn.preprocessing import Binarizer

# 创建特征
age = np.array([[6],
                [12],
                [20],
                [36],
                [65]])

# 创建二值化器
binarizer = Binarizer(18)

# 转换特征
binarizer.fit_transform(age)

# array([[0],
#        [0],
#        [1],
#        [1],
#        [1]])
```

第二种方法是根据多个阈值将数值型特征离散化 ：

```python
# 将特征离散化
np.digitize(age, bins=[20,30,64])

# array([[0],
#        [0],
#        [1],
#        [2],
#        [3]], dtype=int64)
```

记住， bins 参数中的每个数字表示的是每个区间的左边界（左闭右开）。例如， 20 表示第一个区间不包括值为 20 的元素，只包括两个值小于 20 的元素。可以设置 right 参数为 True 来改变这个行为 ：

```python
# 将特征离散化
np.digitize(age, bins=[20,30,64], right=True)

# array([[0],
#        [0],
#        [0],
#        [2],
#        [3]], dtype=int64)
```

如果有足够的理由认为某个数值型特征应该被视为一个分类特征（categorical feature），那么离散化会是一个卓有成效的策略。例如， 19 岁和 20 岁的人消费习惯差距很小，但是 20 岁和 21 岁（在美国， 21 岁的年轻人就可以饮酒了）的人之间消费习惯差距会很大。在这个例子中，将人群划分成能喝酒的和不能喝酒的会很有用。同样，在其他情况下，将数据离散化为 3 个或更多区间也很有用。

在上述解决方案中，我们可以看到两种离散化方法 ：针对 2 个区间使用了 scikit-learn 的Binarizer，针对 3 个及以上的区间使用了 NumPy 的 digitize。 其实，只指定一个阈值，digitize 也能像 Binarizer 一样二值化特征 ：

```python
# 将特征离散化
np.digitize(age, bins=[18])

# array([[0],
#        [0],
#        [1],
#        [1],
#        [1]], dtype=int64)
```

### 10. 使用聚类的方式将观察值分组

如果你有 k 个分组，可以使用 K-Means（K 均值）聚类法将相似的观察值分到一个组，并输出一个新的特征，以标识观察值属于哪一组 ：

```python
# 加载库
import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# 创建模拟的特征矩阵
features, _ = make_blobs(n_samples = 50,
                         n_features = 2,
                         centers = 3,
                         random_state = 1)

# 创建数据帧
dataframe = pd.DataFrame(features, columns=["feature_1","feature_2"])

# 创建 K-Means 聚类器
clusterer = KMeans(3, random_state=0)

# 将聚类器应用在特征上
clusterer.fit(features)

# 预测聚类的值
dataframe["group"] = clusterer.predict(features)

# 查看前几个观察值
dataframe.head(5)

# feature_1 feature_2 group
# 0 -9.877553 -3.336145 0
# 1 -7.287210 -8.353986 2
# 2 -6.943061 -7.023744 2
# 3 -7.440167 -8.791959 2
# 4 -6.641388 -8.075888 2
```

在这里讨论聚类有点早了，本书在后面会更深入地介绍聚类算法。其实，这么早提到聚类主要是因为它可以作为预处理的一个步骤。明确地说，就是使用类似 K-Means 的无监督学习算法将观察值聚类成几组。最后得到一个分类特征，相似的观察值会被分到同一组。

### 11. 删除带有缺失值的观察值

用一行 NumPy 代码就能轻松删除带缺失值的观察值 ：

```python
# 加载库
import numpy as np

# 创建特征矩阵
features = np.array([[1.1, 11.1],
                     [2.2, 22.2],
                     [3.3, 33.3],
                     [4.4, 44.4],
                     [np.nan, 55]])

# 只保留没有（用 ~ 来表示）缺失值的观察值
features[~np.isnan(features).any(axis=1)]

# array([[ 1.1, 11.1],
#        [ 2.2, 22.2],
#        [ 3.3, 33.3],
#        [ 4.4, 44.4]])
```

或者，使用 pandas 丢弃有缺失值的观察值 ：

```python
# 加载库
import pandas as pd

# 加载数据
dataframe = pd.DataFrame(features, columns=["feature_1","feature_2"])

# 删除带有缺失值的观察值
dataframe.dropna()

# feature_1 feature_2
# 0 1.1 11.1
# 1 2.2 22.2
# 2 3.3 33.3
# 3 4.4 44.4
```

大多数机器学习算法不允许目标值或特征数组中存在缺失值。因此，不能简单地忽略数据中的缺失值，而是要在数据预处理阶段解决这个问题。

最简单的解决方法是删除所有含缺失值的观察值，用 NumPy 或者 pandas 很容易实现。即便如此，删除带缺失值的观察值也是一件令人心痛的决定。这样做会让算法丢失观察值中那些非缺失值的信息，所以删除观察值只能作为最终别无他法时不得已的选择。

还有一点很重要，删除观察值可能会在数据中引入偏差，这主要由缺失值的成因决定。缺失值一共有三种类型 ：

完全随机缺失（Missing Completely At Random， MCAR）：数据缺失的可能性与任何其他东西无关。例如，某个接受问卷调查的人会在回答问题前先掷一个骰子，如果掷出了 6，她就跳过那个问题不做回答。

随机缺失（Missing At Random ， MAR）：数据缺失的可能性不是完全随机的，与已经存在的其他特征有关。例如，在一次问卷调查中会问及性别和薪资水平，那么接受调查的女性更可能会跳过薪资的问题，但是她们选择是否不作答要看我们是否已经得知其性别信息。

完全非随机缺失（Missing Not At Random， MNAR）：数据缺失的可能性完全是非随机的，并且与未在特征中反映出的信息有关。例如，一个问卷调查会问到被调查人的薪资水平，而女性更可能会跳过关于薪资水平的问题，但是我们的数据中没有关于性别的特征。

如果观察值是 MCAR 或者 MAR，那么有时候删除它们是可接受的。但是如果它们是MNAR，那么数据缺失本身其实就是一个信息。删除 MNAR 观察值会导致数据产生偏差，因为这些观察值是由未观察到的系统效应产生的。

### 12. 填充缺失值

数据中存在缺失值，我们希望填充或者预测这些值。如果数据量不大，可以使用 KNN（K-Nearest Neighbors， K 近邻）算法来预测缺失值 ：

```python
# 加载库
import numpy as np
from fancyimpute import KNN
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

# 创建模拟特征矩阵
features, _ = make_blobs(n_samples = 1000,
                         n_features = 2,
                         random_state = 1)

# 标准化特征
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# 将第一个特征向量的第一个值替换为缺失值
true_value = standardized_features[0,0]
standardized_features[0,0] = np.nan

# 预测特征矩阵中的缺失值
features_knn_imputed = KNN(k=5, verbose=0).fit_transform(standardized_features)

# 对比真实值和填充值
print("True Value:", true_value)
print("Imputed Value:", features_knn_imputed[0,0])

# True Value: 0.8730186114
# Imputed Value: 1.09553327131
```

还可以使用 scikit-learn 的 Imputer模块，用特征的平均值、中位数或者众数来填充缺失值，不过效果通常都会比使用 KNN 的差 ：

```python
# 加载库
from sklearn.preprocessing import Imputer

# 创建填充器
mean_imputer = Imputer(strategy="mean", axis=0)

# 填充缺失值
features_mean_imputed = mean_imputer.fit_transform(features)

# 对比真实值和填充值
print("True Value:", true_value)
print("Imputed Value:", features_mean_imputed[0,0])

# True Value: 0.8730186114
# Imputed Value: -3.05837272461
```

填充缺失值的策略主要有两种，每种策略各有优缺点。首先，可以使用机器学习来预测缺失值。为了达到目的，可以将带有缺失值的特征当作一个目标向量，然后使用剩余的特征来预测缺失值。虽然可以使用各种机器学习算法来做预测，但是最流行的选择是KNN。我们会在第 14 章中详细介绍 KNN，这里仅做简单的讲解。作为一种机器学习算法，KNN 使用 k 个最近的观察值（根据某种距离度量方法计算出来的）来预测缺失值。上述解决方案中使用了 5 个最近的观察值来预测缺失值。

KNN 的不足是，为了知道哪些观察值离缺失值最近，需要计算每一个观察值与缺失值之间的距离。对于小数据集，这么做没有问题，但是如果数据集中有成千上万的观察值，计算量将成为一个很严重的问题。

一个比较容易扩展到大数据集的替代方案是用平均值来填充缺失值。例如，在我们的解决方案中，借助 scikit-learn 使用特征的平均值来填充缺失值。尽管这样做的效果通常没有使用 KNN 的好，但是"平均值填充策略"很容易扩展到包含成千上万观察值的大数据集。

如果要采用填充策略，最好创建一个二元特征来表明该观察值是否包含填充值。

