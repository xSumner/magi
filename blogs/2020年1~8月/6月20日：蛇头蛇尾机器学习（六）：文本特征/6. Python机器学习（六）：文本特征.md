### 1. 简介

非结构化文本数据是处理起来最有意思也是最难的特征之一，本文会介绍几种将文本转换成富信息特征的数据预处理方法。

### 2. 清洗文本

大部分非结构化文本的基本清洗操作都可以使用 Python 的常用字符串操作完成，其中 strip、replace 和 split 这三个操作用得最多 ：

```python
# 创建文本
text_data = [" Interrobang. By Aishwarya Henriette ",
             "Parking And Going. By Karl Gautier",
             " Today Is The night. By Jarek Prakash "]

# 去除文本两端的空格
strip_whitespace = [string.strip() for string in text_data]

# 查看文本
strip_whitespace

['Interrobang. By Aishwarya Henriette',
 'Parking And Going. By Karl Gautier',
 'Today Is The night. By Jarek Prakash']

# 删除句点
remove_periods = [string.replace(".", "") for string in strip_whitespace]

# 查看文本
remove_periods

# ['Interrobang By Aishwarya Henriette',
#  'Parking And Going By Karl Gautier',
#  'Today Is The night By Jarek Prakash']
```

也可以创建并应用自定义的转换函数 ：

```python
# 创建函数
def capitalizer(string: str) -> str:
    return string.upper()

# 应用函数
[capitalizer(string) for string in remove_periods]

# ['INTERROBANG BY AISHWARYA HENRIETTE',
#  'PARKING AND GOING BY KARL GAUTIER',
#  'TODAY IS THE NIGHT BY JAREK PRAKASH']
```

还可以使用正则表达式来做一些复杂的字符串操作 ：

```python
# 加载库
import re

# 创建函数
def replace_letters_with_X(string: str) -> str:
    return re.sub(r"[a-zA-Z]", "X", string)

# 应用函数
[replace_letters_with_X(string) for string in remove_periods]

# ['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',
#  'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',
#  'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']
```



### 3. 解析并清洗 HTML

Beautiful Soup 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库，可以用它对 HTML 进行解析并提取数据 ：

```python
# 加载库
from bs4 import BeautifulSoup

# 创建一些 HTML 代码
html = """
<div class='full_name'><span style='font-weight:bold'>
Masego Azra"
"""

# 解析 HTML
soup = BeautifulSoup(html, "lxml")

# 查找 class 是 "full_name" 的 div 标签，并查看文本
soup.find("div", { "class" : "full_name" }).text

# '\nMasego Azra"\n'
```

通常 Beautiful Soup是用于抓取在线网页的，不过它也可以很轻松地提取嵌在HTML中的文本。

### 4. 移除标点

移除文本数据的特征中的标点。可以定义一个函数，将一个标点字典作为参数传入 translate ：

```python
# 加载库
import unicodedata
import sys

# 创建文本
text_data = ['Hi!!!! I. Love. This. Song....',
             '10000% Agree!!!! #LoveIT',
             'Right?!?!']

# 创建一个标点字典
punctuation = dict.fromkeys(i for i in range(sys.maxunicode)
                            if unicodedata.category(chr(i)).startswith('P'))

# 移除每个字符串中的标点
[string.translate(punctuation) for string in text_data]

# ['Hi I Love This Song', '10000 Agree LoveIT', 'Right']
```

translate 因其非凡的性能成为 Python 中非常流行的函数。上述解决方案先创建一个punctuation 字典，将 Unicode 中的标点字符作为 key， None 作为 value，然后将字符串中所有在 punctuation 字典中出现过的字符（即标点）转换成 None，高效地移除它们。当然还有更易于理解的移除标点的方案，这个方案有点像黑客解决方案，但是它远快于其他的方案。

标点也包含信息（对比一下“是？”和“是！”），认识到这个事实是很重要的。在创建特征时难免要删除标点，但是如果这个标点很重要，还是要考虑将其保留。

### 5. 文本分词

将文本分离成独立的单词。Python 的自然语言工具集（Natural Language Toolkit， NLTK）在处理文本方面有很多功能强大的操作，其中包括文本分词（word tokenizing）：

```python
# 加载库
from nltk.tokenize import word_tokenize

# 创建文本
string = "The science of today is the technology of tomorrow"

# 分词
word_tokenize(string)

['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']
```

也可以将文本切分为句子 ：

```python
# 加载库
from nltk.tokenize import sent_tokenize

# 创建文本
string = "The science of today is the technology of tomorrow. Tomorrow is today."

# 切分成句子
sent_tokenize(string)

# ['The science of today is the technology of tomorrow.', 'Tomorrow is today.']
```

文本分词，尤其是将文本分为独立的单词是在清洗文本数据之后的常见任务，因为它是将文本转换成能用于构建有效特征的数据的第一步。

### 6. 删除停止词（stop word）

给定一组分好词的文本数据，使用 NLTK 的 stopwords删除其中非常常见但包含的信息又很少的单词（例如， a、 is、 of、 on）：

```python
# 加载库
from nltk.corpus import stopwords

# 如果是第一次使用停止词集，则需要先下载
# import nltk
# nltk.download('stopwords')

# 创建单词序列
tokenized_words = ['i',
                   'am',
                   'going',
                   'to',
                   'go',
                   'to',
                   'the',
                   'store',
                   'and',
                   'park']

# 加载停止词
stop_words = stopwords.words('english')

# 删除停止词
[word for word in tokenized_words if word not in stop_words]

# ['going', 'go', 'store', 'park']
```

尽管“停止词”可以指代所有需要在数据预处理阶段删除的单词，但是这个术语常常用来指代那些特别常见而包含的信息又很少的单词。 NLTK 有一个常见停止词列表，可用来查找并删除单词序列中的停止词 ：

```python
# 查看停止词
stop_words[:5]

# ['i', 'me', 'my', 'myself', 'we']
```

注意， NLTK 的 stopwords 假设所有的单词都是小写形式的。

### 7. 提取词干

使用 NLTK 的 PorterStemmer 将一个单词序列中的单词转换成它们的词干：

```python
# 加载库
from nltk.stem.porter import PorterStemmer

# 创建单词序列
tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']

# 创建词干转换器
porter = PorterStemmer()

# 应用词干转换器
[porter.stem(word) for word in tokenized_words]

# ['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']
```

词干提取（stemming）能识别出一个单词的词缀并将其删除（例如，动名词中的“ing”后缀），同时保留其词根的意思，以此得到这个词的词干。例如，“tradition”和“traditional”的词干都是“tradit”，表明虽然它们是不同的单词，但基本意思是相同的。提取文本的词干后，单词的可读性会变差，不过也更接近它的基本意思，因此更适合用来做比较。

NLTK 的 PorterStemmer 实现了被广泛使用的波特词干（Porter stemming）算法，移除或替换单词中常用的后缀来生成词干。

### 8. 标注词性

使用 NLTK 预训练的词性标注器，为文本数据中的每个单词或字符标注词性 ：

```python
# 加载库
from nltk import pos_tag
from nltk import word_tokenize

# 创建文本
text_data = "Chris loved outdoor running"

# 使用预训练的词性标注器
text_tagged = pos_tag(word_tokenize(text_data))

# 查看词性
text_tagged

# [('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]
```

上述代码输出一个包含单词和其词性的元组列表。 NLTK 使用 Penn Treebank 的词性标签进行标注。这里给出一部分 Penn Treebank 标签的例子，如下表所示。

| 标签 |            词性            |
| :--: | :------------------------: |
| NNP  |        单数专有名词        |
|  NN  |      单数或复数的名词      |
|  RB  |            副词            |
| VBD  |        过去式的动词        |
| VBG  | 动名词或动词的现在分词形式 |
|  JJ  |           形容词           |
| PRP  |         人称代词6          |

一旦为文本添加了词性标签，就能依据标签找到特定词性的单词了。例如，找到所有的名词 ：

```python
# 过滤单词
[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]

# ['Chris']
```

现实中更可能遇到的情况是，有一份数据，其中每个观察值都包含一条推文，我们想将这些句子转换成用词性表示的特征（例如，如果是专有名词，特征值就是 1，否则为 0）：

```python
import nltk
from sklearn.preprocessing import MultiLabelBinarizer
# 创建文本
tweets = ["I am eating a burrito for breakfast",
          "Political science is an amazing field",
          "San Francisco is an awesome city"]

# 创建列表
tagged_tweets = []

# 为每条推文中的每个单词加标签
for tweet in tweets:
    tweet_tag = nltk.pos_tag(word_tokenize(tweet))
    tagged_tweets.append([tag for word, tag in tweet_tag])
    
# 使用 one-hot 编码将标签转换成特征
one_hot_multi = MultiLabelBinarizer()
one_hot_multi.fit_transform(tagged_tweets)

# array([[1, 1, 0, 1, 0, 1, 1, 1, 0],
#        [1, 0, 1, 1, 0, 0, 0, 0, 1],
#        [1, 0, 1, 1, 1, 0, 0, 0, 1]])
```

使用 classes_ 就能发现每个特征都是一个词性标签 ：

```python
# 查看特征名
one_hot_multi.classes_

# array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'], dtype=object)
```

如果文本是英文的，而且不是关于某个特定主题的（例如医学），那么标注词性最简单的做法就是使用 NLTK 预训练的词性标注器。但是，如果 pos_tag 不是很准确， NLTK 也提供了自行训练标注器的机制。训练标注器的主要缺点就是需要一个很大的文本语料库，并且其中每个单词的词性标签都是已知的。构建这样的带标签的语料库显然是一件很烦琐的工作，所以只能作为最后没有办法的办法。

说了这么多，如果你想要用带标签的语料库来训练标注器，可以参照下面例子的做法。我们使用的语料库是 Brown Copus（布朗语料库），它是最流行的带标签的文本语料库之一。我们使用的是回退 n 元模型（back n-gram）标注器， n 表示在预测一个单词的词性时需要考虑多少个前面的单词（其实是 n-1 个）。首先，考虑这个单词的前两个单词（使用 TrigramTagger），如果它前面没有两个单词，则“回退”到只考虑它前面的一个单词，这时候使用 BigramTagger。如果这个单词前面一个单词都没有，就通过 UnigramTagger只考虑这个单词本身。为了检测标注器的准确率，我们将文本数据分成两个部分，一部分用来训练标注器，另一部分用来测试标注器的效果 ：

```python
# 加载库
from nltk.corpus import brown
from nltk.tag import UnigramTagger
from nltk.tag import BigramTagger
from nltk.tag import TrigramTagger

# 从布朗语料库中获取文本数据，切分成句子
sentences = brown.tagged_sents(categories='news')

# 将 4000 个句子用作训练， 623 个句子用作测试
train = sentences[:4000]
test = sentences[4000:]

# 创建回退标注器
unigram = UnigramTagger(train)
bigram = BigramTagger(train, backoff=unigram)
trigram = TrigramTagger(train, backoff=bigram)

# 查看准确率
trigram.evaluate(test)

# 0.8174734002697437
```

### 9. 将文本编码成词袋（Bag of Words）

使用 scikit-learn 的 CountVectorizer 创建一组特征来表示观察值文本中包含的特定单词的数量 ：

```python
# 加载库
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

# 创建文本
text_data = np.array(['I love Brazil. Brazil!',
                      'Sweden is best',
                      'Germany beats both'])

# 创建一个词袋特征矩阵
count = CountVectorizer()
bag_of_words = count.fit_transform(text_data)

# 查看特征矩阵
bag_of_words

# <3x8 sparse matrix of type '<class 'numpy.int64'>'
# 	with 8 stored elements in Compressed Sparse Row format>
```

上述代码的输出是一个稀疏矩阵。当文本量很大时，使用这样的存储方式是有必要的。
但是在我们的玩具例子中，可以使用 toarray 查看每个观察值的词频统计矩阵 ：

```python
bag_of_words.toarray()

# array([[0, 0, 0, 2, 0, 0, 1, 0],
#        [0, 1, 0, 0, 0, 1, 0, 1],
#        [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)
```

使用 get_feature_names 方法能查看每个特征所对应的单词 ：

```python
# 查看特征名
count.get_feature_names()

# ['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']
```

这样的输出可能有点令人困惑。为了说明特征矩阵到底是什么样子的，下面以单词为列名（每一行就是一个观察值）给出了具体示例。

```python
beats best both brazil germany is love sweden
0 0 0 2 0 0 1 0
0 1 0 0 0 1 0 1
1 0 1 0 1 0 0 0
```

将文本转换成特征的最常用的方法之一就是使用词袋模型（bag-of-words model）。词袋模型为文本数据中的每一个单词都输出一个特征，每个特征都包含该单词在观察值中出现的次数。例如，在我们的解决方案中，因为 brazil 在句子“I love Brazil. Brazil!”中出现了两次，所以特征“brazil”的值是 2。

上述解决方案中的文本数据有意选得很小，但是在现实场景中，一个文本数据的观察值可能是整本书的内容！因为词袋模型为数据中的每个单词都创建了一个特征，所以生成的矩阵会包含成千上万个特征，也就意味着这个矩阵有时需要占用很大的内存。但幸运的是，我们可以利用词袋特征矩阵的特性来减少需要存储的数据量。

大多数观察值并不会包含太多单词，因此词袋特征矩阵里会有很多值为 0。这样的矩阵被称为“稀疏”矩阵。可以只存储非零值并假设所有其他值都是 0，而不是将矩阵中的所有值都存储下来，这样当特征矩阵很大时就能节省很多内存空间。 CountVectorizer 有一个很好的特性，就是默认以稀疏矩阵作为输出。

CountVectorizer 还有很多有用的参数，它们能让创建词袋特征矩阵变得很简单。首先，尽管默认情况下每个特征是一个单词，但也并非必须如此。我们可以将特征设置成两个词（称为二元模型， 2-gram）或三个词（三元模型， 3-gram）的组合。 ngram_range 为我们的 n 元模型设置最大元和最小元，例如， (2,3) 会返回所有的二元模型和三元模型。其次，使用 stop_words 也可以很方便地移除信息量比较少的单词，停止词列表可以是函数自带的也可以是自定义的。最后，使用 vocabulary 可以将观察值限定为仅在特定的单词表中出现过的单词或短语。例如，创建一个只包含国家名字的词袋特征矩阵 ：

```python
# 用特定参数创建特征矩阵
count_2gram = CountVectorizer(ngram_range=(1,2),
                              stop_words="english",
                              vocabulary=['brazil'])
bag = count_2gram.fit_transform(text_data)

# 查看特征矩阵
bag.toarray()

# array([[2],
#        [0],
#        [0]], dtype=int64)

# 查看一元模型和二元模型
count_2gram.vocabulary_

# {'brazil': 0}
```

### 10. 按单词的重要性加权

对词袋模型中的词按照其在观察值中的重要程度进行加权。使用 TF-IDF（term frequency-inverse document frequency ）将一个词在某个文档（推文、影评、演讲稿等）中的出现次数和这个词在所有文档中的出现次数进行对比。用 scikitlearn 的 TfidfVectorizer 能很方便地做这个对比 ：

```python
# 加载库
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

# 创建文本
text_data = np.array(['I love Brazil. Brazil!',
                      'Sweden is best',
                      'Germany beats both'])

# 创建 TF-IDF 特征矩阵
tfidf = TfidfVectorizer()
feature_matrix = tfidf.fit_transform(text_data)

# 查看 TF-IDF 特征矩阵
feature_matrix

# <3x8 sparse matrix of type '<class 'numpy.float64'>'
# 	with 8 stored elements in Compressed Sparse Row format>
```

和上一节一样， TfidfVectorizer 的输出也是一个稀疏矩阵。不过，如果想看稠密矩阵形式的输出，可以使用 .toarray ：

```python
# 查看 TF-IDF 特征矩阵的稠密矩阵形式
feature_matrix.toarray()

# array([[0. , 0. , 0. , 0.89442719, 0. , 0. , 0.4472136 , 0. ],
#        [0. , 0.57735027, 0. , 0. , 0. , 0.57735027, 0.4472136 , 0.57735027],
#        [0.57735027, 0. , 0.57735027, 0. , 0.57735027, 0. , 0. , 0. ]])
```

vocabulary_ 可以查看特征的单词表 ：

```python
# 查看特征的名字
tfidf.vocabulary_

# {'beats': 0,
#  'best': 1,
#  'both': 2,
#  'brazil': 3,
#  'germany': 4,
#  'is': 5,
#  'love': 6,
#  'sweden': 7}
```

一个词在文档中出现的次数越多，它对这个文档就越重要。例如，如果单词 economy 在某篇文档中频繁出现，就表示这个文档可能是关于经济的。我们把单词在文档中的出现次数称为词频（term frequency， tf）。

相反，如果一个单词在很多文档中都出现了，那么它对于单个文档的重要性就没有那么高了。例如，如果每个文档中的文本数据都包含单词 after，那么它很可能不是一个重要的词。一个单词出现在了多少文档中，我们把这个数值称为文档频率（document frequency， df）。

将这两个统计值组合起来，我们就能为每个单词确定一个分数，来表示它在某个文档中的重要程度了。具体地说，就是将 tf 和 idf（inverse of document frequency，逆向文档频率）相乘 ：
$$
\text{tf}-\text{idf}(t, d) = \text{tf}(t, d)·\text{idf}(t)
$$
这里， $t$ 表示单词， $d$ 表示文档。计算 tf 和 idf 的方法有很多。在 scikit-learn 中，就是单词在文档中出现的次数， idf 是这样计算出来的 ：
$$
\text{idf}(t) = \log{\frac{1 + n_d}{1 + \text{df}(d, t)}} + 1
$$
$n_d$ 是文档的数量， $\text{df}(d,t)$ 是单词 $t$ 的文档频率（也就是单词在多少份文档中出现过）。

默认情况下， scikit-learn 会使用欧氏范数（L2 范数）将 TF-IDF 向量归一化。得到的结果值越大，这个单词对一个文档来说就越重要。



